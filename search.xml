<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2022/08/22/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/1-SparkPlan%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6/"/>
      <url>/2022/08/22/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/1-SparkPlan%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="1-SparkPlan生成框架"><a href="#1-SparkPlan生成框架" class="headerlink" title="1-SparkPlan生成框架"></a>1-SparkPlan生成框架</h1><h2 id="0-准备知识"><a href="#0-准备知识" class="headerlink" title="0. 准备知识"></a>0. 准备知识</h2><p>RDD(Resilient Distributed Datasets)，直译为弹性分布式数据集，是spark中最核心的概念。RDD是对数据的虚拟抽象，本身不包含真实的数据。RDD详细描述可参考<a href="http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf">论文</a>，这里不做过多介绍。</p><p>spark的计算流程：</p><ol><li>数据集通过spark接口转换成RDD，例如：<code>textFile</code>，<code>hadoopFile</code>，<code>binaryFiles</code>等</li><li>根据业务逻辑，使用transform算子（<code>map</code>，<code>filter</code>，<code>flatMap</code>，<code>union</code>，<code>distinct</code>，<code>reduceByKey</code>等）、action算子（<code>reduce</code>，<code>collect</code>，<code>count</code>，<code>first</code>，<code>saveAsTextFile</code>等）将RDD组装成DAG（有向无环图）。可以简单理解是数据的加工计算链路。</li><li>调用action算子时，数据通过上述的DAG链路，计算结果后输出。</li></ol><p>以最熟悉的word count为例，计算流程如图：</p><p><img src="https://raw.githubusercontent.com/huldarchen/node_image/master/2022/image_1661101103725_0.png" alt="image_1661101103725_0"></p><h2 id="1-SparkPlan是什么"><a href="#1-SparkPlan是什么" class="headerlink" title="1. SparkPlan是什么"></a>1. SparkPlan是什么</h2><h3 id="1-1-SparkPlan是什么"><a href="#1-1-SparkPlan是什么" class="headerlink" title="1.1 SparkPlan是什么"></a>1.1 SparkPlan是什么</h3><p>一条SQL经过parser转化成LogicalPlan（逻辑执行计划），LogicalPlan经过Analyzer、Optimizer解析、优化后，生成更适合计算的LogicalPlan。LogicalPlan简单理解为数据大概的（逻辑上的）计算过程（<em>类比设计师设计的图纸</em>），不能真正的运行，因为缺少必须的要素，如数据（RDD）以及具体的算子的执行算法等。SparkPlan就是要将计算缺少的要素具象化（<em>工人根据图纸拆分成具体的操作流程</em>）。</p><p>SparkPlan的继承关系:</p><p><img src="https://raw.githubusercontent.com/huldarchen/node_image/master/2022/image_1661130985638_0.png" alt="image_1661130985638_0"></p><p>SparkPlan是一棵树，树的叶子节点（<code>LeafExecNode</code>子类）的<code>doExecute</code>，<code>execute*</code>方法生成第一个RDD，非叶子节点（<code>UnaryExecNode</code>，<code>BinaryExecNode</code>）的<code>doExecute</code>，<code>execute*</code>方法表示了当前节点数据计算的算法（<code>transform</code>算子），根节点表示计算结果的输出方式（<code>action</code>算子）。</p><p>简而言之，<strong>SparkPlan就是可以交给spark框架执行的一颗树</strong>。</p><blockquote><p>实际上通过<code>QueryExecution.createSparkPlan()</code>生成的SparkPlan还不能直接运行，还需要经过<code>QueryExecution.prepareForExecution()</code>增加一些shuffle操作和内部行格式转换才能最终交给spark执行。但转换后还是SparkPlan类。</p></blockquote><h3 id="1-2-SparkPlan做了什么"><a href="#1-2-SparkPlan做了什么" class="headerlink" title="1.2 SparkPlan做了什么"></a>1.2 SparkPlan做了什么</h3><p>想要了解SparkPlan到底做了什么，我们通过查看其方法就可以得到。SparkPlan类的主要方法有：</p><p> a. 计算相关</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">final</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">execute</span></span>(): <span class="hljs-type">RDD</span>[<span class="hljs-type">InternalRow</span>]; <span class="hljs-comment">// 最终委托给doExecute()方法</span><br><span class="hljs-keyword">protected</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doExecute</span></span>(): <span class="hljs-type">RDD</span>[<span class="hljs-type">InternalRow</span>];<br><br><span class="hljs-keyword">final</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">executeBroadcast</span></span>[<span class="hljs-type">T</span>](): broadcast.<span class="hljs-type">Broadcast</span>[<span class="hljs-type">T</span>]; <span class="hljs-comment">// 最终委托给doExecuteBroadcast()方法</span><br><span class="hljs-keyword">protected</span>[sql] <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doExecuteBroadcast</span></span>[<span class="hljs-type">T</span>](): broadcast.<span class="hljs-type">Broadcast</span>[<span class="hljs-type">T</span>];<br><br><span class="hljs-keyword">final</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">executeColumnar</span></span>(): <span class="hljs-type">RDD</span>[<span class="hljs-type">ColumnarBatch</span>]<br><span class="hljs-keyword">protected</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doExecuteColumnar</span></span>(): <span class="hljs-type">RDD</span>[<span class="hljs-type">ColumnarBatch</span>]<br></code></pre></td></tr></table></figure><p>b. 分区与排序</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 分区，输入数据与输出数据的分区情况，以此来判断是否要进行shuffle</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outputPartitioning</span></span>: <span class="hljs-type">Partitioning</span> <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">requiredChildDistribution</span></span>: <span class="hljs-type">Seq</span>[<span class="hljs-type">Distribution</span>]<br><br><span class="hljs-comment">// 排序相关，输入数据与输出数据的排序规则</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outputOrdering</span></span>: <span class="hljs-type">Seq</span>[<span class="hljs-type">SortOrder</span>] <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">requiredChildOrdering</span></span>: <span class="hljs-type">Seq</span>[<span class="hljs-type">Seq</span>[<span class="hljs-type">SortOrder</span>]]<br></code></pre></td></tr></table></figure><p>c. 其他方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 运行计算指标信息，进行监控等</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">metrics</span></span>: <span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">SQLMetric</span>]<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resetMetrics</span></span>(): <span class="hljs-type">Unit</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">longMetric</span></span>(name: <span class="hljs-type">String</span>): <span class="hljs-type">SQLMetric</span><br></code></pre></td></tr></table></figure><p><strong>总结</strong>：SparkPlan的作用主要有3点，</p><ol><li>描述当前数据怎么进行计算（算法）</li><li>描述当前数据上下游分区、排序情况，来判断是否进行shuffle与排序</li><li>描述当前计划的指标与元数据</li></ol><h2 id="2-SparkPlan如何生成"><a href="#2-SparkPlan如何生成" class="headerlink" title="2. SparkPlan如何生成"></a>2. SparkPlan如何生成</h2><h3 id="2-1-生成入口"><a href="#2-1-生成入口" class="headerlink" title="2.1 生成入口"></a>2.1 生成入口</h3><p>LogicalPlan生成SparkPlan是在<code>QueryExecution#sparkPlan</code>属性懒加载时。不同类型的操作触发时机不同，分命令类和查询类。</p><p>命令类SQL&#x2F;DSL（dataset 风格）在初始化Dataset的时候，执行<code>Dataset.commandExecuted</code>属性初始化时触发sparkPlan初始化，具体流程如下图：</p><p><img src="https://raw.githubusercontent.com/huldarchen/node_image/master/2022/image_1661151179146_0.png" alt="image_1661151179146_0"></p><p>查询类SQL&#x2F;DSL，SparkPlan初始化比较简单，在执行action操作的时候，调用DataSet的<code>withAction</code>方法触发sparkPlan初始化。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 查询类操作，DataSet.scala</span><br><span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">withAction</span></span>[<span class="hljs-type">U</span>](name: <span class="hljs-type">String</span>, qe: <span class="hljs-type">QueryExecution</span>)(action: <span class="hljs-type">SparkPlan</span> =&gt; <span class="hljs-type">U</span>) = &#123;<br>  <span class="hljs-type">SQLExecution</span>.withNewExecutionId(qe, <span class="hljs-type">Some</span>(name)) &#123;<br>    <span class="hljs-type">QueryExecution</span>.withInternalError(<span class="hljs-string">s&quot;&quot;</span><span class="hljs-string">&quot;The &quot;</span>$<span class="hljs-string">name&quot; action failed.&quot;</span><span class="hljs-string">&quot;&quot;</span>) &#123;<br>      qe.executedPlan.resetMetrics() <span class="hljs-comment">// 这里触发生成SparkPlan初始化</span><br>      action(qe.executedPlan)<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-2-SparkPlan生成框架"><a href="#2-2-SparkPlan生成框架" class="headerlink" title="2.2 SparkPlan生成框架"></a>2.2 SparkPlan生成框架</h3><p>SparkPlan的生成框架核心代码是在<code>QueryPlanner.plan</code>方法。具体代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plan</span></span>(plan: <span class="hljs-type">LogicalPlan</span>): <span class="hljs-type">Iterator</span>[<span class="hljs-type">PhysicalPlan</span>] = &#123;<br>  <span class="hljs-keyword">val</span> candidates = strategies.iterator.flatMap(_ (plan)) <span class="hljs-comment">// 生成候选集</span><br>  <span class="hljs-keyword">val</span> plans = candidates.flatMap &#123; candidate =&gt;<br>    <span class="hljs-keyword">val</span> placeholders = collectPlaceholders(candidate)<br>    <span class="hljs-keyword">if</span> (placeholders.isEmpty) &#123;<br>      <span class="hljs-type">Iterator</span>(candidate)<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>      placeholders.iterator.foldLeft(<span class="hljs-type">Iterator</span>(candidate)) &#123;<br>        <span class="hljs-keyword">case</span> (candidatesWithPlaceholders, (placeholder, logicalPlan)) =&gt;<br>          <span class="hljs-keyword">val</span> childPlans = <span class="hljs-keyword">this</span>.plan(logicalPlan) <span class="hljs-comment">// 递归自上而下生成候选集</span><br>          candidatesWithPlaceholders.flatMap &#123; candidateWithPlaceholders =&gt;<br>            childPlans.map &#123; childPlan =&gt;<br>              candidateWithPlaceholders.transformUp &#123;<br>                <span class="hljs-keyword">case</span> p <span class="hljs-keyword">if</span> p.eq(placeholder) =&gt; childPlan <span class="hljs-comment">// 自下而上进行替换</span><br>              &#125;<br>            &#125;<br>          &#125;<br>      &#125;<br>    &#125;<br>  &#125;<br>  <span class="hljs-keyword">val</span> pruned = prunePlans(plans)<br>  assert(pruned.hasNext, <span class="hljs-string">s&quot;No plan for <span class="hljs-subst">$plan</span>&quot;</span>)<br>  pruned<br>&#125;<br></code></pre></td></tr></table></figure><p>以<code>select b,a from testdata2 where b&lt;2</code>SQL为例，逐步分析生成SparkPlan过程。</p><p>先上结果，左边为优化后的LogicalPlan，右边为生成的SparkPlan。（添加Exec为了进行区分，实际上是没有）</p><figure class="highlight cal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cal">+- Project [b<span class="hljs-string">#4</span>, a<span class="hljs-string">#3</span>]                |  Project(Exec) [b<span class="hljs-string">#4</span>, a<span class="hljs-string">#3</span>]<br>   +- Filter (b<span class="hljs-string">#4</span> &lt; <span class="hljs-number">2</span>)               |   +- Filter(Exec) (b<span class="hljs-string">#4</span> &lt; <span class="hljs-number">2</span>)<br>      +- SerializeFromObject [...]   |      +- SerializeFromObject(Exec) [...]<br>         +- ExternalRDD [obj<span class="hljs-string">#2</span>]      |         +- Scan(ExternalRDDScanExec)[obj<span class="hljs-string">#2</span>]<br></code></pre></td></tr></table></figure><blockquote><p>先停下来思考下，如果这个执行框架是自己实现，该怎么样做呢？</p></blockquote><p>spark中通过两个特殊的节点<code>ReturnAnswer</code>和<code>PlanLater</code>（占位符）实现替换逻辑。上述SQL转换过程如图（分析递归调用栈）：</p><p><img src="https://raw.githubusercontent.com/huldarchen/node_image/master/2022/physicalPlan-%E6%89%A7%E8%A1%8C%E6%A1%86%E6%9E%B6.png" alt="physicalPlan-执行框架"></p><p>生成框架总结：<strong>自上而下生成SparkPlan，自下而上进行组装成树</strong>。</p><h2 id="3-小结"><a href="#3-小结" class="headerlink" title="3. 小结"></a>3. 小结</h2><h3 id="3-1-LogicalPlan生成SparkPlan"><a href="#3-1-LogicalPlan生成SparkPlan" class="headerlink" title="3.1 LogicalPlan生成SparkPlan"></a>3.1 LogicalPlan生成SparkPlan</h3><p>LogicalPlan生成SparkPlan本质上是由一棵树转换成另一颗树，自上而下通过strategies策略将当前LogicalPlan节点转换成SparkPlan，然后自下而上替换Planeholder（占位符）组装SparkPlan树。</p><p>LogicalPlan生成SparkPlan目的是补充计算要素，最终能够在框架中进行执行。</p><h3 id="3-2-小知识点"><a href="#3-2-小知识点" class="headerlink" title="3.2 小知识点"></a>3.2 小知识点</h3><ol><li>scala的flatMap不是立即触发function逻辑，而是在调用其他方法（如：next，hasNext等）触发具体function逻辑执行</li><li>strategies包含的策略中<code>SpecialLimits</code>策略唯一处理<code>ReturnAnswer</code>节点。</li><li><code>ReturnAnswer</code>节点是特殊的SparkPlan，表示根节点。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/07/29/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/%E9%9A%8F%E8%AE%B0/"/>
      <url>/2022/07/29/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/%E9%9A%8F%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/06/27/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/00-sparksql%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/"/>
      <url>/2022/06/27/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/00-sparksql%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="TreeNode体系"><a href="#TreeNode体系" class="headerlink" title="TreeNode体系"></a>TreeNode体系</h2><p>TreeNode有两大体系，查询计划「QueryPlan」（逻辑执行计划和物理执行计划）和表达式。执行计划主要是表达SQL中from、join等SQL关键词的语法，表达式是除了表示的是剩余所有。</p><h3 id="QueryPlan"><a href="#QueryPlan" class="headerlink" title="QueryPlan"></a>QueryPlan</h3><p>![image-20220628110320998](&#x2F;Users&#x2F;huldarchen&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220628110320998.png)</p><h4 id="1-logical-plan"><a href="#1-logical-plan" class="headerlink" title="1. logical plan"></a>1. logical plan</h4><p>SQL语法解析后的AST转换而成，Unresolved和Resolved两条线，根据子节点数量分为叶子节点、一元节点、二元节点、多元节点（直接继承LogicalPlan，如Union）。根据SQL类型分为命令式Command和query。</p><ul><li>叶子节点主要是表示表信息（relation）</li><li>一元节点包含aggregate、distinct、sort、window、view、Filter、limit、subquery、hint、with、expand、project等</li><li>二元节点包含join、CoGroup、Except、Intersect等</li><li>多元节点包含union等</li></ul><h4 id="2-spark-plan"><a href="#2-spark-plan" class="headerlink" title="2. spark plan"></a>2. spark plan</h4><p>spark plan表示物理执行计划。通过Resolved logical plan生成。体系构成和logical plan基本一致，带有Exec后缀。</p><h3 id="Expression"><a href="#Expression" class="headerlink" title="Expression"></a>Expression</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/06/17/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/07-%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82BHJ/"/>
      <url>/2022/06/17/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/07-%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82BHJ/</url>
      
        <content type="html"><![CDATA[<h2 id="JOIN方式有哪些？"><a href="#JOIN方式有哪些？" class="headerlink" title="JOIN方式有哪些？"></a>JOIN方式有哪些？</h2><p>从spark的JoinSelection中可以看到有5中join：</p><ul><li><p>Broadcast hash join（BHJ）</p></li><li><p>Shuffle hash join（SHJ）</p></li><li><p>Shuffle sort merge join（SMJ）</p></li><li><p>Broadcast nested loop join（BNLJ）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 因为我们下面测试数据都很小，所以我们先把 BroadcastJoin 关闭</span><br><br>scala&gt; spark.conf.set(<span class="hljs-string">&quot;spark.sql.autoBroadcastJoinThreshold&quot;</span>, <span class="hljs-number">1</span>)<br><br><br>scala&gt; <span class="hljs-keyword">val</span> iteblogDF1 = <span class="hljs-type">Seq</span>(<br><br>     |   (<span class="hljs-number">0</span>, <span class="hljs-string">&quot;111&quot;</span>),<br><br>     |   (<span class="hljs-number">1</span>, <span class="hljs-string">&quot;222&quot;</span>),<br><br>     |   (<span class="hljs-number">2</span>, <span class="hljs-string">&quot;333&quot;</span>)<br><br>     | ).toDF(<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;info&quot;</span>)<br><br>iteblogDF1: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [id: int, info: string]<br><br><br>scala&gt; <span class="hljs-keyword">val</span> iteblogDF2 = <span class="hljs-type">Seq</span>(<br><br>     |   (<span class="hljs-number">0</span>, <span class="hljs-string">&quot;https://www.iteblog.com&quot;</span>),<br><br>     |   (<span class="hljs-number">1</span>, <span class="hljs-string">&quot;iteblog_hadoop&quot;</span>),<br><br>     |   (<span class="hljs-number">2</span>, <span class="hljs-string">&quot;iteblog&quot;</span>)<br><br>     | ).toDF(<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;info&quot;</span>)<br><br>iteblogDF2: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [id: int, info: string]<br><br><br>scala&gt; <span class="hljs-keyword">val</span> r = iteblogDF1.join(iteblogDF2, <span class="hljs-type">Nil</span>, <span class="hljs-string">&quot;leftouter&quot;</span>)<br><br>r: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [id: int, info: string ... <span class="hljs-number">2</span> more fields]<br><br><br>scala&gt; r.explain<br><br>== <span class="hljs-type">Physical</span> <span class="hljs-type">Plan</span> ==<br><br><span class="hljs-type">BroadcastNestedLoopJoin</span> <span class="hljs-type">BuildRight</span>, <span class="hljs-type">LeftOuter</span><br><br>:- <span class="hljs-type">LocalTableScan</span> [id#<span class="hljs-number">157</span>, info#<span class="hljs-number">158</span>]<br><br>+- <span class="hljs-type">BroadcastExchange</span> <span class="hljs-type">IdentityBroadcastMode</span>, [id=#<span class="hljs-number">516</span>]<br><br>   +- <span class="hljs-type">LocalTableScan</span> [id#<span class="hljs-number">168</span>, info#<span class="hljs-number">169</span>]<br><br><br>scala&gt; r.show(<span class="hljs-literal">false</span>)<br><br>+---+----+---+-----------------------+<br><br>|id |info|id |info                   |<br><br>+---+----+---+-----------------------+<br><br>|<span class="hljs-number">0</span>  |<span class="hljs-number">111</span> |<span class="hljs-number">0</span>  |https:<span class="hljs-comment">//www.iteblog.com|</span><br><br>|<span class="hljs-number">0</span>  |<span class="hljs-number">111</span> |<span class="hljs-number">1</span>  |iteblog_hadoop         |<br><br>|<span class="hljs-number">0</span>  |<span class="hljs-number">111</span> |<span class="hljs-number">2</span>  |iteblog                |<br><br>|<span class="hljs-number">1</span>  |<span class="hljs-number">222</span> |<span class="hljs-number">0</span>  |https:<span class="hljs-comment">//www.iteblog.com|</span><br><br>|<span class="hljs-number">1</span>  |<span class="hljs-number">222</span> |<span class="hljs-number">1</span>  |iteblog_hadoop         |<br><br>|<span class="hljs-number">1</span>  |<span class="hljs-number">222</span> |<span class="hljs-number">2</span>  |iteblog                |<br><br>|<span class="hljs-number">2</span>  |<span class="hljs-number">333</span> |<span class="hljs-number">0</span>  |https:<span class="hljs-comment">//www.iteblog.com|</span><br><br>|<span class="hljs-number">2</span>  |<span class="hljs-number">333</span> |<span class="hljs-number">1</span>  |iteblog_hadoop         |<br><br>|<span class="hljs-number">2</span>  |<span class="hljs-number">333</span> |<span class="hljs-number">2</span>  |iteblog                |<br><br>+---+----+---+-----------------------+<br></code></pre></td></tr></table></figure></li><li><p>Shuffle-and-replicate nested loop join（笛卡尔积join）</p></li></ul><h2 id="1-什么是BHJ？"><a href="#1-什么是BHJ？" class="headerlink" title="1. 什么是BHJ？"></a>1. 什么是BHJ？</h2><p>Broadcast hash join（BHJ）又称map-side-only join，join发生在map端，也就要求两个表有一个小表，数据小到能够放在driver端和executor端。</p><p>sparkBHJ的实现是，将小表数据收集到Dirver端，然后通过sparkContext.broadcast方法将小表数据广播到executor端内存中，避免在join时候executor端通过网络拉取小表数据，提高join效率【大部分情况】。具体执行流程：</p><p><img src="/Users/huldarchen/Documents/blog/huldarchen.github.io/source/_posts/modb_20211108_fec0e73e-4028-11ec-bb72-38f9d3cd240d.png" alt="img"></p><p>那么spark什么时候确定使用BHJ？确定后输出的是什么？满足什么样的条件才能使用BHJ呢？</p><h2 id="2-spark的BHJ判断逻辑"><a href="#2-spark的BHJ判断逻辑" class="headerlink" title="2. spark的BHJ判断逻辑"></a>2. spark的BHJ判断逻辑</h2><p>sparkSQL执行过程<br><img src="/Users/huldarchen/Documents/blog/huldarchen.github.io/source/_posts/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pjMTk5MjEyMTU=,size_16,color_FFFFFF,t_70.png" alt="img"></p><ul><li><p>什么时候确定？</p><p>确定join策略发生在Optimizated Logical Plan转化Physical Plans过程中，是JoinSelection策略进行确定。</p></li><li><p>确定的输出结果？</p><p>确定后输出 BuildSide，包含两个类型BuildRight和BuildLeft。代表要广播那边的表。</p></li><li><p>满足什么条件？</p><ol><li><p>有hint<br>broadcastjoin的hint包含：<code>BROADCAST</code>、<code>BROADCASTJOIN</code>、<code>MAPJOIN</code>。用法示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">/*+ BROADCAST(t1) */</span><br><span class="hljs-comment">/*+ BROADCAST(tmp1, tmp2) */</span> <span class="hljs-comment">-- 多个表hint使用，分割</span><br><span class="hljs-comment">/*+ BROADCASTJOIN (t1) */</span><br><span class="hljs-comment">/*+ MAPJOIN(t2) */</span><br></code></pre></td></tr></table></figure><p>使用hint指定后，先会判断左表和右表那个有hint，然后会根据join类型判断是否支持广播，支持的join类型：</p><table><thead><tr><th>表位置</th><th>支持的join</th></tr></thead><tbody><tr><td>右表</td><td>inner join<br />right outer join</td></tr><tr><td>左表</td><td>inner join<br />left outer join<br />left semi join<br />left anti join<br />ExistenceJoin</td></tr></tbody></table><p>如果 左表 和 右表都有hint，且join 类型为 inner join，会计算左右表的数据量，右表&lt;&#x3D;左表：广播右表，否则广播左表。</p></li><li><p>无hint</p><ol><li>判断数据量（数据加载到内存中的大小，要比实际表文件大小要大）是否小于广播的阈值<br>广播的阈值在3.2以后可以通过2个参数进行控制：<ul><li>spark.sql.adaptive.autoBroadcastJoinThreshold 开启AQE时生效，单位byte</li><li>spark.sql.autoBroadcastJoinThreshold 默认是10M，也作为兜底。</li></ul></li><li>不存在<code>no_broadcast_hash</code>的hint</li><li>join type是否支持 同上。</li></ol><p>其中数据量的判断比较重要，下节单独说明。</p></li></ol></li></ul><h2 id="3-数据量指标解析"><a href="#3-数据量指标解析" class="headerlink" title="3. 数据量指标解析"></a>3. 数据量指标解析</h2><ol><li><p>数据量判断代码体系<br>调用LogicalPlanStats#stats方法获取统计信息Statistics。Statistics有两种生成方式：</p><ul><li>开启CBO    BasicStatsPlanVisitor</li><li>不开启CBO  SizeInBytesOnlyStatsPlanVisitor</li></ul><p>通过访问者的方式对不同的logical plan获取</p><p><img src="/Users/huldarchen/Documents/blog/huldarchen.github.io/source/_posts/image-20220617151429182.png" alt="image-20220617151429182">默认实现default(p)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// BasicStatsPlanVisitor#default </span><br><span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">default</span></span>(p: <span class="hljs-type">LogicalPlan</span>): <span class="hljs-type">Statistics</span> = p <span class="hljs-keyword">match</span> &#123;<br>    <span class="hljs-keyword">case</span> p: <span class="hljs-type">LeafNode</span> =&gt; p.computeStats()<br>    <span class="hljs-keyword">case</span> _: <span class="hljs-type">LogicalPlan</span> =&gt;<br>      <span class="hljs-keyword">val</span> stats = p.children.map(_.stats)<br>      <span class="hljs-keyword">val</span> rowCount = <span class="hljs-keyword">if</span> (stats.exists(_.rowCount.isEmpty)) &#123;<br>        <span class="hljs-type">None</span><br>      &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-type">Some</span>(stats.map(_.rowCount.get).filter(_ &gt; <span class="hljs-number">0</span>L).product)<br>      &#125;<br>      <span class="hljs-type">Statistics</span>(sizeInBytes = stats.map(_.sizeInBytes).filter(_ &gt; <span class="hljs-number">0</span>L).product, rowCount = rowCount)<br>  &#125;<br><span class="hljs-comment">// SizeInBytesOnlyStatsPlanVisitor#default</span><br>  <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">default</span></span>(p: <span class="hljs-type">LogicalPlan</span>): <span class="hljs-type">Statistics</span> = p <span class="hljs-keyword">match</span> &#123;<br>    <span class="hljs-keyword">case</span> p: <span class="hljs-type">LeafNode</span> =&gt; p.computeStats()<br>    <span class="hljs-keyword">case</span> _: <span class="hljs-type">LogicalPlan</span> =&gt;<br>      <span class="hljs-type">Statistics</span>(sizeInBytes = p.children.map(_.stats.sizeInBytes).filter(_ &gt; <span class="hljs-number">0</span>L).product)<br>  &#125;<br></code></pre></td></tr></table></figure><p>从底表逐层向上调用，最底层调用LeafNode的computeStats方法获取，一元节点</p></li><li><p>常用</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/06/08/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/06-RuleExecutor%E8%AF%A6%E8%A7%A3/"/>
      <url>/2022/06/08/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/06-RuleExecutor%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p><a href="http://rdms.dmall.com/#index/environment/fitenv/appenvconfig:versionType=FIT&amp;tab=test&amp;env=test&amp;appCode=dmall-fit-uniservice&amp;appName=%E7%BB%9F%E4%B8%80%E6%9F%A5%E8%AF%A2%E6%9C%8D%E5%8A%A1&amp;sysCode=uniservice-fit">http://rdms.dmall.com/#index/environment/fitenv/appenvconfig:versionType=FIT&amp;tab=test&amp;env=test&amp;appCode=dmall-fit-uniservice&amp;appName=%E7%BB%9F%E4%B8%80%E6%9F%A5%E8%AF%A2%E6%9C%8D%E5%8A%A1&amp;sysCode=uniservice-fit</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/06/08/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/05-SparkSql%E4%BC%98%E5%8C%96%E5%99%A8/"/>
      <url>/2022/06/08/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/05-SparkSql%E4%BC%98%E5%8C%96%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="1-优化器执行框架"><a href="#1-优化器执行框架" class="headerlink" title="1. 优化器执行框架"></a>1. 优化器执行框架</h2><h3 id="1-1-优化器功能"><a href="#1-1-优化器功能" class="headerlink" title="1.1 优化器功能"></a>1.1 优化器功能</h3><h3 id="1-2-优化器执行流程"><a href="#1-2-优化器执行流程" class="headerlink" title="1.2 优化器执行流程"></a>1.2 优化器执行流程</h3><h3 id="1-3-设计特点"><a href="#1-3-设计特点" class="headerlink" title="1.3 设计特点"></a>1.3 设计特点</h3><h2 id="2-优化器规则有哪些与分类"><a href="#2-优化器规则有哪些与分类" class="headerlink" title="2. 优化器规则有哪些与分类"></a>2. 优化器规则有哪些与分类</h2><h2 id="3-常见优化器规则详解"><a href="#3-常见优化器规则详解" class="headerlink" title="3. 常见优化器规则详解"></a>3. 常见优化器规则详解</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/05/30/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/04-SparkPlan/"/>
      <url>/2022/05/30/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/04-SparkPlan/</url>
      
        <content type="html"><![CDATA[<h1 id="SparkPlan"><a href="#SparkPlan" class="headerlink" title="SparkPlan"></a>SparkPlan</h1><p>SparkPlan的核心是 输入—&gt;输出</p><ol><li>数据从子节点怎么流入当前节点？分区个数，怎么进行分区的</li><li>数据在当前节点怎么进行排序？</li><li>数据通过什么分区方式输出到下一个结点？</li><li>怎么生成RDD[InternalRow]</li></ol><h2 id="1-SparkPlan继承体系与重要属性"><a href="#1-SparkPlan继承体系与重要属性" class="headerlink" title="1. SparkPlan继承体系与重要属性"></a>1. SparkPlan继承体系与重要属性</h2><p>叶子执行节点，一元执行节点，二元执行节点。</p><h2 id="1-2-支持hashAggregation的类型："><a href="#1-2-支持hashAggregation的类型：" class="headerlink" title="1.2 支持hashAggregation的类型："></a>1.2 支持hashAggregation的类型：</h2><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">-- mutable类型集合<br><span class="hljs-built_in">NullType,</span><br><span class="hljs-built_in">BooleanType,</span><br><span class="hljs-built_in">ByteType,</span><br><span class="hljs-built_in">ShortType,</span><br><span class="hljs-built_in">IntegerType,</span><br><span class="hljs-built_in">LongType,</span><br><span class="hljs-built_in">FloatType,</span><br><span class="hljs-built_in">DoubleType,</span><br><span class="hljs-built_in">DateType,</span><br><span class="hljs-built_in">TimestampType,</span><br>-- 单独<br><span class="hljs-built_in">DecimalType,</span><br><span class="hljs-built_in">CalendarIntervalType,</span><br><span class="hljs-built_in">DayTimeIntervalType,</span><br>YearMonthIntervalType<br></code></pre></td></tr></table></figure><p>聚合函数缓冲区：</p><p>聚合函数编写要素（步骤）：</p><ol><li>初始化一个或多个聚合函数缓冲区</li><li>updateExpressions</li><li>mergeExpressions</li><li>evaluateExpression</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/05/24/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/03-Expression%E5%88%86%E6%9E%90/"/>
      <url>/2022/05/24/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/03-Expression%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h2 id="Expression是什么？"><a href="#Expression是什么？" class="headerlink" title="Expression是什么？"></a>Expression是什么？</h2><p>在SQL表达式中除了<code>SELECT</code>、<code>FROM</code> 等关键字以外，其他大部分元素都可以理解成是Expression。例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">SUM</span>(A), B <span class="hljs-keyword">FROM</span> T1 t <span class="hljs-keyword">WHERE</span> t.A <span class="hljs-operator">&gt;</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>其中<code>sum(A)</code>、<code>B</code>、<code>t.A &gt; 0</code>都是Expression。</p><p><strong>Expression是不需要触发执行引擎而能够直接进行计算的单元。</strong></p><p>计算单元：输入—计算—&gt;输出</p><h2 id="Expression有哪些属性和重要方法？"><a href="#Expression有哪些属性和重要方法？" class="headerlink" title="Expression有哪些属性和重要方法？"></a>Expression有哪些属性和重要方法？</h2><p>Expression也是一颗树，继承了TreeNode。从5个方面拆解：</p><ol><li><strong>基本属性</strong><ul><li><code>deterministic</code>：表达式是否为确定性的，即每次执行eval函数的输出是否都相同。 <code>a &gt; b</code>对于给定的输入有同样的输出，<code>rand()</code>对于给定的输入，每次输出结果不一致。主要用于优化阶段，谓词下推。</li><li><code>foldable</code>:true表示表达式在执行查询之前是可以进行静态计算。</li><li><code>resolved</code>、<code>childrenResolved</code>：是否解析了所有的<strong>占位符</strong>以及进行<strong>输入类型</strong>的检查。如果此表达式及其所有子表达式已解析了占位符并且输入数据类型检查通过，则返回“true”，如果仍包含任何未解析的占位符或数据类型不匹配，则返回“false”。<br>children为null的时候返回true。</li><li><code>nullable</code>：<strong>用来标记表达式是否可能输出Null值</strong>，一般在<code>genCode()</code>生成的Java代码中对null值进行判断。</li></ul></li><li><strong>输入输出</strong><ul><li><code>dataType</code>：表达式计算后的输出结果的类型。对于unresolved的表达式是无效的。</li><li><code>checkInputDataTypes</code>：检查输入的数据类型是否合法。当childrenResolved !&#x3D; true的时候这个方法是无效的。</li><li><code>references</code>：表达引用到的列信息。默认为子节点的所有的引用的列进行去重集合<code>AttributeSet</code>。</li><li><code>flatArgnuments</code>：表达式参数扁平化为迭代器。</li></ul></li><li><strong>核心操作</strong><ul><li><code>eval(input: InernalRow = null):Any</code>：对给定的输入行，计算并返回计算结果。</li><li><code>genCode():ExprCode</code>和<code>doGenCode():ExprCode</code>：生成表达式对应的Java可编译的源码。</li></ul></li><li><strong>等价性判断</strong><ul><li><code>canonicalized</code>：表达式规范化。使用树的后续遍历，先对children节点进行规范化，最后规范化当前节点。使用<code>Canonicalize</code>工具类进行规范化。</li><li><code>semanticEquals</code>：两个表达的计算结果是否一致。条件：两个表达式都是确定的，并且规范化后的表达式一致。</li><li><code>semanticHash</code>：规范化后的表达式的hash值</li></ul></li><li>字符串表示：<strong>输出表达式信息</strong></li></ol><h2 id="Expression的继承体系"><a href="#Expression的继承体系" class="headerlink" title="Expression的继承体系"></a>Expression的继承体系</h2><p>Expression在3.2版本中有687个子类。继承体系较为复杂，从属性维度，可以将Expression分为：不确定表达式、不可计算表达式、支持代码生成的表达式；从树的结构（子节点个数）上将Expression分为：叶子表达式、一元、二元、三元、四元、6+1（option）等。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs mermaid">graph LR<br>A(Expression); B([Nondeterministic:不确定表达式]); C([Unevaluable:不可计算表达式]); D([CodegenFallback:支持代码生成的表达式]);E([LeafExpression:叶子节点]);F([UnaryExpression:一元]);G([BinaryExpression:二元]);H([TernaryExpression:三元]);I([QuaternaryExpression:四元]);J([SeptenaryExpression:6+1]);A1(属性角度);A2(结构角度);A3(ComplexTypeMergingExpression复杂类型合并表达式)<br>A --&gt; A1<br>A1 --&gt; B<br>A1 --&gt; C<br>A1 --&gt; D<br>A --&gt; A2<br>A2 --&gt; E<br>A2 --&gt; F<br>A2 --&gt; G<br>A2 --&gt; H<br>A2 --&gt; I<br>A2 --&gt; J<br>A --&gt; A3<br>A3 --&gt; B1([CaseWhenB1])<br>A3 --&gt; B2([If])<br>A3 --&gt; B3([ArrayUnion])<br><br></code></pre></td></tr></table></figure><h2 id="常用Expression详解"><a href="#常用Expression详解" class="headerlink" title="常用Expression详解"></a>常用Expression详解</h2><ol><li><p>UnresolvedAttribute: 尚未解析的属性（字段,叶子节点），从名字上能看出来<code>resolved = false</code>，且不可计算。nameParts是对列的标识，<code>[t1,c1]</code>即<code>t1.c1</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">UnresolvedAttribute</span>(<span class="hljs-params">nameParts: <span class="hljs-type">Seq</span>[<span class="hljs-type">String</span>]</span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Attribute</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Unevaluable</span></span><br></code></pre></td></tr></table></figure></li><li><p>UnresolvedStar：尚未解析的通配符<code>*</code>（叶子节点），<code>target</code>代表了要拓展的表名或者结构体名。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">UnresolvedStar</span>(<span class="hljs-params">target: <span class="hljs-type">Option</span>[<span class="hljs-type">Seq</span>[<span class="hljs-type">String</span>]]</span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Star</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Unevaluable</span></span><br></code></pre></td></tr></table></figure></li><li><p>UnresolvedFunction：标识未解析的函数。是一个不可计算的Expression。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">UnresolvedFunction</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-class">    nameParts: <span class="hljs-type">Seq</span>[<span class="hljs-type">String</span>], // 函数名称</span></span><br><span class="hljs-params"><span class="hljs-class">    arguments: <span class="hljs-type">Seq</span>[<span class="hljs-type">Expression</span>], // 函数参数</span></span><br><span class="hljs-params"><span class="hljs-class">    isDistinct: <span class="hljs-type">Boolean</span>,// 结果是否是distinct</span></span><br><span class="hljs-params"><span class="hljs-class">    filter: <span class="hljs-type">Option</span>[<span class="hljs-type">Expression</span>] = <span class="hljs-type">None</span>, // 前置过滤</span></span><br><span class="hljs-params"><span class="hljs-class">    ignoreNulls: <span class="hljs-type">Boolean</span> = false</span>)<span class="hljs-comment">// 是否忽视null值</span></span><br>  <span class="hljs-keyword">extends</span> <span class="hljs-type">Expression</span> <span class="hljs-keyword">with</span> <span class="hljs-type">Unevaluable</span> &#123;<br></code></pre></td></tr></table></figure></li><li><p>Alias：代表别名。是一元表达式。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Alias</span>(<span class="hljs-params">child: <span class="hljs-type">Expression</span>, // 子节点表达式</span></span><br><span class="hljs-params"><span class="hljs-class">                 name: <span class="hljs-type">String</span> // 别名</span></span><br><span class="hljs-params"><span class="hljs-class">                </span>)(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-class">    val exprId: <span class="hljs-type">ExprId</span> = <span class="hljs-type">NamedExpression</span>.newExprId,</span></span><br><span class="hljs-params"><span class="hljs-class">    val qualifier: <span class="hljs-type">Seq</span>[<span class="hljs-type">String</span>] = <span class="hljs-type">Seq</span>.empty,</span></span><br><span class="hljs-params"><span class="hljs-class">    val explicitMetadata: <span class="hljs-type">Option</span>[<span class="hljs-type">Metadata</span>] = <span class="hljs-type">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-class">    val nonInheritableMetadataKeys: <span class="hljs-type">Seq</span>[<span class="hljs-type">String</span>] = <span class="hljs-type">Seq</span>.empty</span>)</span><br>  <span class="hljs-keyword">extends</span> <span class="hljs-type">UnaryExpression</span> <span class="hljs-keyword">with</span> <span class="hljs-type">NamedExpression</span> &#123;<br></code></pre></td></tr></table></figure></li><li><p>数值运算表达式：<code>Add</code>、<code>Abs</code>、<code>Subtract</code>、<code>Divide</code>等等位于arithmetic.scala文件中。</p></li><li><p>逻辑表达式：位于predicates.scala文件中。</p><ul><li>逻辑运算：<code>And</code>、<code>Or</code>、<code>Not</code></li><li>数值比较：<code>EqualTo</code>、<code>LessThan</code>、<code>GreaterThan</code>等等</li></ul></li><li><p>空值表达式：位于nullExpression.scala文件中</p><ul><li>判断：<code>IsNaN</code>、<code>IsNull</code>、<code>IsNotNull</code></li><li>提取：<code>Coalesce</code>、<code>Nvl</code>、<code>Nvl2</code>等等</li></ul></li></ol><p>其他表达式不一一例举。 </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/05/19/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/02-Analyzer%E8%A7%84%E5%88%99%E6%98%8E%E7%BB%86/"/>
      <url>/2022/05/19/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/02-Analyzer%E8%A7%84%E5%88%99%E6%98%8E%E7%BB%86/</url>
      
        <content type="html"><![CDATA[<h2 id="规则"><a href="#规则" class="headerlink" title="规则"></a>规则</h2><table><thead><tr><th>分类</th><th>遍历次数</th><th>规则名称</th><th>作用</th><th>备注</th></tr></thead><tbody><tr><td>Substitution</td><td>fixedPoint</td><td>OptimizeUpdateFields</td><td>此规则优化了“UpdateFields”表达式链，因此看起来更像优化规则。然而，当操作深度嵌套的模式时，<code>UpdateFields</code>表达式树可能非常复杂，无法进行分析。因此，我们需要在分析之初就优化“UpdateFields”。</td><td>前序遍历</td></tr><tr><td>Substitution</td><td>fixedPoint</td><td>CTESubstitution</td><td>根据以下条件，使用节点进行分析，并用CTE参考或CTE定义替换子计划：<br />1.如果处于传统模式，或者如果查询是SQL命令或DML语句，请替换为CTE定义，即内联CTE。<br />2.否则，替换为CTE的references的ctrelationref‘s。在查询分析之后，将由规则<code>InlineCTE</code>决定是否内联。<br />对于每个主查询和子查询，此替换后未内联的所有CTE定义都将分组在一个<code>WithCTE</code>节点下。任何不包含CTE或已内联所有CTE的主查询或子查询显然都不会有任何<code>WithCTE</code>节点。如果有的话，<code>WithCTE</code>节点将与最外层的<code>With</code>节点所在的位置相同。<br /><code>WithCTE</code>节点中的CTE定义按解析顺序保存。这意味着，根据CTE定义对任何有效CTE查询的依赖性，可以保证CTE定义按拓扑顺序排列（即，给定CTE定义A和B，B引用A，A保证出现在B之前）。否则，它必须是无效的用户查询，关系解析规则稍后将抛出分析异常。</td><td>逻辑比较复杂</td></tr><tr><td>Substitution</td><td>fixedPoint</td><td>WindowsSubstitution</td><td>用 WindowSpecDefinitions 替代子计划，其中 WindowSpecDefinition 代表的是窗口函数的规范。</td><td>前序遍历</td></tr><tr><td>Substitution</td><td>fixedPoint</td><td>EliminateUnions</td><td>如果只有一个子项，则从计划中删除 Union 算子</td><td></td></tr><tr><td>Substitution</td><td>fixedPoint</td><td>SubstituteUnresolvedOrdinals</td><td>将<code>order by</code>或<code>group by</code>中的<code>Literal</code>序号替换为UnresolvedOrdinal表达式，其中UnresolvedOrdinal表示按order by或group by使用的未解析序号。例如：<code>select a from table order by 1 </code>和 <code>select a from table group by 1</code></td><td></td></tr><tr><td>Disable Hints</td><td>One</td><td>ResolveHints.DisableHints</td><td>当配置项spark.sql.optimizer.disableHints被设置时删除spark时的所有hints。这将在Analyzer的最开始执行，以禁用hints功能。</td><td></td></tr><tr><td>Hints</td><td>fixedPoint</td><td>ResolveHints.ResolveJoinStrategyHints</td><td>joinHints处理</td><td></td></tr><tr><td>Hints</td><td>fixedPoint</td><td>ResolveHints.ResolveCoalesceHints</td><td>重分区Hints处理</td><td></td></tr><tr><td>Simple Sanity Check</td><td>One</td><td>LookupFunctions</td><td>检查函数是否注册<br />检查函数注册表中是否定义了 UnresolvedFunction 引用的函数标识符。请注意，此规则不会尝试解析 UnresolvedFunction。它仅根据函数标识符执行简单的存在性检查，以快速识别未定义的函数，而不触发关系解析，这在某些情况下可能会导致潜在的昂贵的分区模式发现过程。为了避免重复的外部函数查找，外部函数标识符将存储在本地哈希集externalFunctionNameSet中</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveTableValuedFunctions</td><td>解析表值函数引用的规则。</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveNamespace</td><td>解析诸如SHOW TABLES、SHOW FUNCTIONS之类的规则。SHOW TABLES&#x2F;SHOW TABLE EXTENDED&#x2F;SHOW VIEWS&#x2F;SHOW FUNCTIONS&#x2F;ANALYZE TABLES</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveCatalogs</td><td>从 SQL 语句中的多部分标识符解析目录，如果解析的目录不是会话目录，则将语句转换为相应的 v2 命令。</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveUserSpecifiedColumns</td><td>解析用户指定的列。</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveInsertInto</td><td>解析INSERT INTO语句。</td><td></td></tr><tr><td><strong>Resolution</strong></td><td><strong>fixedPoint</strong></td><td><strong>ResolveRelations</strong></td><td>用catalog中的具体关系替换未解析的关系（表和视图），比如：包含UnresolvedRelation节点</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveTables</td><td>用v2目录中的具体关系解析表关系。[[ResolveRelations]]仍然解析v1表</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolvePartitionSpec</td><td>在分区相关命令中将 UnresolvedPartitionSpec 解析为 ResolvedPartitionSpec。</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveAlterTableCommands</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>AddMetadataColumns</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>DeduplicateRelations</td><td></td><td></td></tr><tr><td><strong>Resolution</strong></td><td><strong>fixedPoint</strong></td><td><strong>ResolveReferences</strong></td><td><strong>将 UnresolvedAttributes 替换为来自逻辑计划节点的子节点的具体 AttributeReferences</strong></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveExpressionsWithNamePlaceholders</td><td>如果表达式包含 NamePlaceholder，则解析表达式。</td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveDeserializer</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveNewInstance</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveUpCast</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveGroupingAnalytics</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolvePivot</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveOrdinalInOrderByAndGroupBy</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveAggAliasInGroupBy</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveMissingReferences</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ExtractGenerator</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveGenerate</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveFunctions</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveAliases</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveSubquery</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveSubqueryColumnAliases</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveWindowOrder</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveWindowFrame</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveNaturalAndUsingJoin</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveOutputRelation</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ExtractWindowExpressions</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>GlobalAggregates</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveAggregateFunctions</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>TimeWindowing</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>SessionWindowing</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveInlineTables</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveHigherOrderFunctions(catalogManager)</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveLambdaVariables</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveTimeZone</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveRandomSeed</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveBinaryArithmetic</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveUnion</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>typeCoercionRules</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>ResolveWithCTE</td><td></td><td></td></tr><tr><td>Resolution</td><td>fixedPoint</td><td>extendedResolutionRules</td><td></td><td></td></tr><tr><td>Remove TempResolvedColumn</td><td>Once</td><td>RemoveTempResolvedColumn</td><td></td><td></td></tr><tr><td>Apply Char Padding</td><td>Once</td><td>ApplyCharTypePadding</td><td></td><td></td></tr><tr><td>Post-Hoc Resolution</td><td>Once</td><td>ResolveCommandsWithIfExists</td><td></td><td></td></tr><tr><td>Remove Unresolved Hints</td><td>Once</td><td>ResolveHints.RemoveAllHints</td><td></td><td></td></tr><tr><td>Nondeterministic</td><td>Once</td><td>PullOutNondeterministic</td><td></td><td></td></tr><tr><td>UDF</td><td>Once</td><td>HandleNullInputsForUDF</td><td></td><td></td></tr><tr><td>UDF</td><td>Once</td><td>ResolveEncodersInUDF</td><td></td><td></td></tr><tr><td>UpdateNullability</td><td>Once</td><td>UpdateAttributeNullability</td><td></td><td></td></tr><tr><td>Subquery</td><td>Once</td><td>UpdateOuterReferences</td><td></td><td></td></tr><tr><td>Cleanup</td><td>Once</td><td>CleanupAliases</td><td></td><td></td></tr><tr><td>HandleAnalysisOnlyCommand</td><td>Once</td><td>HandleAnalysisOnlyCommand</td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/05/19/scala%E6%8A%80%E5%B7%A7/ClassTag%E5%92%8CTypeTag%E6%AF%94%E8%BE%83/"/>
      <url>/2022/05/19/scala%E6%8A%80%E5%B7%A7/ClassTag%E5%92%8CTypeTag%E6%AF%94%E8%BE%83/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/05/19/scala%E6%8A%80%E5%B7%A7/TypeTag%E7%94%A8%E6%B3%95/"/>
      <url>/2022/05/19/scala%E6%8A%80%E5%B7%A7/TypeTag%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/05/19/scala%E6%8A%80%E5%B7%A7/ClassTag%E7%94%A8%E6%B3%95/"/>
      <url>/2022/05/19/scala%E6%8A%80%E5%B7%A7/ClassTag%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="ClassTag用法"><a href="#ClassTag用法" class="headerlink" title="ClassTag用法"></a>ClassTag用法</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/05/06/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/01-sparksql%E8%AF%8D%E6%B3%95%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/"/>
      <url>/2022/05/06/spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/01-sparksql%E8%AF%8D%E6%B3%95%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/04/26/mysql-client-server-protocol/"/>
      <url>/2022/04/26/mysql-client-server-protocol/</url>
      
        <content type="html"><![CDATA[<h2 id="mysql客户端和服务端协议"><a href="#mysql客户端和服务端协议" class="headerlink" title="mysql客户端和服务端协议"></a>mysql客户端和服务端协议</h2><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>mysql协议被用于客户端和服务端之间。它被实施于：</p><ul><li>连接器（connector&#x2F;c，connector&#x2F;j等等）</li><li>MySQL代理</li><li>主从副本服务间通信</li></ul><p>这个协议支持一下功能：</p><p>- </p><blockquote><p>Caution</p><p>If the first byte of a packet is a length-encoded integer and its byte value is <code>0xfe</code>, you must check the length of the packet to verify that it has enough space for a 8-byte integer.</p><p>If not, it may be an <a href="https://dev.mysql.com/doc/internals/en/packet-EOF_Packet.html"><code>EOF_Packet</code></a> instead.</p></blockquote><h3 id="String-types"><a href="#String-types" class="headerlink" title="String types"></a>String types</h3><p>字符串是字节序列，有在协议中有几种形式：</p><ul><li><p>Protocol::FiexedLengthString</p><p>实现 string&lt;fix&gt;</p></li><li><p>Protocol::NulTerminatedString</p></li></ul><p>​    实现 string&lt;nul&gt;</p><ul><li>Protocol::VariableLengthString</li></ul><p>​    实现 string&lt;var&gt;</p><ul><li>Protocol::LengthEncodedString</li></ul><p>​    长度编码字符串是以长度编码整数为前缀的字符串，该整数描述字符串的长度。</p><p>​    它是Protocol::VariableLengthString的一种特殊情况</p><p>​    它的属性是：<code>length(int&lt;lenenc&gt;)</code>和<code>string(string&lt;fix&gt;)</code></p><p>​    实现是 string&lt;lenenc&gt;</p><ul><li><p>Protocol::RestOfPacketString</p><p>如果一个字符串是一个包的最后一个部分，它的长度可以从包的总长度减去当前位置来计算。</p><p>实现 string&lt;EOF&gt;</p></li></ul><h3 id="通用的响应包"><a href="#通用的响应包" class="headerlink" title="通用的响应包"></a>通用的响应包</h3><h4 id="OK-packet"><a href="#OK-packet" class="headerlink" title="OK packet"></a>OK packet</h4><p>![image-20220426143005601](&#x2F;Users&#x2F;huldarchen&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220426143005601.png)</p><p>这些规则区分数据包是代表 OK 还是 EOF：</p><ul><li>OK: <code>header</code> &#x3D; 0 and length of packet &gt; 7</li><li>EOF: <code>header</code> &#x3D; 0xfe and length of packet &lt; 9</li></ul><h3 id="ERR-Packet"><a href="#ERR-Packet" class="headerlink" title="ERR_Packet"></a>ERR_Packet</h3><p>![image-20220426145412874](&#x2F;Users&#x2F;huldarchen&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220426145412874.png)</p><h3 id="EOF-Packet"><a href="#EOF-Packet" class="headerlink" title="EOF_Packet"></a>EOF_Packet</h3><p>![image-20220426145519601](&#x2F;Users&#x2F;huldarchen&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220426145519601.png)</p><h3 id="status-flags"><a href="#status-flags" class="headerlink" title="status flags"></a>status flags</h3><p>![image-20220426145648323](&#x2F;Users&#x2F;huldarchen&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220426145648323.png)</p><p>![image-20220426165016315](&#x2F;Users&#x2F;huldarchen&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220426165016315.png)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/04/13/UniService-Feature-SQL%E8%A7%A3%E6%9E%90/"/>
      <url>/2022/04/13/UniService-Feature-SQL%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="UniService-Feature-SQL解析"><a href="#UniService-Feature-SQL解析" class="headerlink" title="UniService-Feature-SQL解析"></a>UniService-Feature-SQL解析</h1><h2 id="1-参考软件功能"><a href="#1-参考软件功能" class="headerlink" title="1. 参考软件功能"></a>1. 参考软件功能</h2><h3 id="1-1-druid"><a href="#1-1-druid" class="headerlink" title="1.1. druid"></a>1.1. druid</h3><ol><li>通过jdbcURL来判断是什么数据源</li><li>独立解析服务</li></ol><h3 id="1-2-presto"><a href="#1-2-presto" class="headerlink" title="1.2. presto"></a>1.2. presto</h3><p>![image-20220413170935283](&#x2F;Users&#x2F;huldarchen&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220413170935283.png)</p><ol><li>使用Antlr进行SQL解析</li><li>通过catalog来判断是什么数据源</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>durid sql parse</title>
      <link href="/2022/04/13/durid-sql-parse/"/>
      <url>/2022/04/13/durid-sql-parse/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>What Every Programmer Should Know About Memory</title>
      <link href="/2022/04/11/What-Every-Programmer-Should-Know-About-Memory/"/>
      <url>/2022/04/11/What-Every-Programmer-Should-Know-About-Memory/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>spark-antlr4解析sql生成树</title>
      <link href="/2022/04/11/spark-antlr4%E8%A7%A3%E6%9E%90sql%E7%94%9F%E6%88%90%E6%A0%91/"/>
      <url>/2022/04/11/spark-antlr4%E8%A7%A3%E6%9E%90sql%E7%94%9F%E6%88%90%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h2 id="sql解析"><a href="#sql解析" class="headerlink" title="sql解析"></a>sql解析</h2><p>解析sql生成logic plan，是从最底层开始创建plan，然后一层一层网上挂。</p><p>window进行去重：org.apache.spark.sql.catalyst.analysis.Analyzer.ExtractWindowExpressions#addWindow</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/04/06/hello-world/"/>
      <url>/2022/04/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
    
  
</search>
